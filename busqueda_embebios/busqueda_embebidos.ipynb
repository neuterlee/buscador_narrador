{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código implementa un sistema de búsqueda basado en embeddings densos generados a partir de descripciones en un archivo CSV. A continuación, se describen sus principales funciones y estructura:\n",
    "\n",
    "## Funcionalidad\n",
    "1. **Carga de Datos**:\n",
    "   - Se utiliza un archivo CSV (`data/repd_vp_cedulas_principal.csv`) que contiene descripciones en la columna `descripcion_desaparicion`.\n",
    "   - Opcionalmente, se puede filtrar el CSV para excluir filas específicas y guardar el resultado en `data/filtered_dataset.csv`.\n",
    "\n",
    "2. **Generación de Embeddings**:\n",
    "   - Se utiliza el modelo `BGEM3FlagModel` para generar embeddings densos de las descripciones.\n",
    "   - Los embeddings se guardan en caché (`data/database_embeddings.pkl` o `data/filtered_embeddings.pkl`) para evitar recalcularlos.\n",
    "\n",
    "3. **Búsqueda**:\n",
    "   - El usuario ingresa una consulta en lenguaje natural.\n",
    "   - El sistema genera un embedding para la consulta y calcula la similitud con los embeddings precomputados.\n",
    "   - Devuelve los resultados más similares (top-k) con sus puntajes.\n",
    "\n",
    "4. **Exportación de Resultados**:\n",
    "   - Los resultados se exportan en dos formatos:\n",
    "     - **HTML**: Un archivo visual (`output/search_results.html`) que muestra los resultados con detalles.\n",
    "     - **CSV**: Un archivo (`output/filtered_results.csv`) que contiene las filas originales del CSV relacionadas con los resultados.\n",
    "\n",
    "5. **Carpeta de Salida**:\n",
    "   - Todos los archivos generados (HTML y CSV) se guardan en la carpeta `output`. Si no existe, el código la crea automáticamente.\n",
    "\n",
    "## Estructura de Carpetas y Archivos\n",
    "- **Entrada**:\n",
    "  - `data/repd_vp_cedulas_principal.csv`: Archivo CSV principal.\n",
    "  - `data/filtered_dataset.csv`: (Opcional) CSV filtrado.\n",
    "- **Caché**:\n",
    "  - `data/database_embeddings.pkl`: Caché de embeddings.\n",
    "  - `data/filtered_embeddings.pkl`: Caché de embeddings filtrados.\n",
    "- **Salida**:\n",
    "  - `output/search_results_YYYYMMDD_HHMMSS.html`: Resultados en HTML.\n",
    "  - `output/filtered_results_YYYYMMDD_HHMMSS.csv`: Resultados en CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisitos\n",
    "- Instalar las dependencias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install pandas torch sentence-transformers jinja2 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "import hashlib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "\n",
    "def ensure_output_folder_exists():\n",
    "    \"\"\"\n",
    "    Ensures that the 'output' folder exists.\n",
    "    \"\"\"\n",
    "    output_folder = \"output\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    return output_folder\n",
    "\n",
    "def set_memory_limit(memory_fraction=0.9):\n",
    "    \"\"\"\n",
    "    Sets a memory limit for GPU usage to avoid crashes.\n",
    "    :param memory_fraction: Fraction of GPU memory to allocate (default is 50%).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_per_process_memory_fraction(memory_fraction)\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU memory usage capped at {memory_fraction * 100:.0f}%.\")\n",
    "\n",
    "def compute_file_hash(file_path):\n",
    "    \"\"\"\n",
    "    Computes the MD5 hash of a file to detect changes.\n",
    "    \"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "def load_cached_embeddings(cache_file, file_hash):\n",
    "    \"\"\"\n",
    "    Loads cached embeddings if the file hash matches.\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "            if cache.get(\"file_hash\") == file_hash:\n",
    "                print(\"Loaded cached embeddings.\")\n",
    "                return cache.get(\"embeddings\"), cache.get(\"texts\")\n",
    "    return None, None\n",
    "\n",
    "def save_embeddings_to_cache(cache_file, file_hash, embeddings, texts):\n",
    "    \"\"\"\n",
    "    Saves embeddings and their corresponding texts to a cache file along with the file hash.\n",
    "    \"\"\"\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump({\"file_hash\": file_hash, \"embeddings\": embeddings, \"texts\": texts}, f)\n",
    "    print(\"Saved embeddings to cache.\")\n",
    "\n",
    "def apply_filter(input_csv, filtered_csv):\n",
    "    \"\"\"\n",
    "    Applies a filter to the original CSV file and saves the filtered data to a new CSV file.\n",
    "    :param input_csv: Path to the original CSV file.\n",
    "    :param filtered_csv: Path to save the filtered CSV file.\n",
    "    :return: Path to the filtered CSV file.\n",
    "    \"\"\"\n",
    "    print(\"Applying filter to the dataset...\")\n",
    "    cases = []\n",
    "    with open(input_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            # Filter cases that meet the conditions\n",
    "            if (row['condicion_localizacion'].strip().upper() == \"NO APLICA\"):\n",
    "                cases.append(row)\n",
    "\n",
    "    # Save the filtered cases to a new CSV file\n",
    "    with open(filtered_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(cases)\n",
    "\n",
    "    print(f\"Filtered dataset saved to {filtered_csv}\")\n",
    "    return filtered_csv\n",
    "\n",
    "def generate_embeddings_for_database(input_csv, column, cache_file):\n",
    "    \"\"\"\n",
    "    Generates dense embeddings for the specified column in the database and caches them.\n",
    "    :param input_csv: Path to the input CSV file.\n",
    "    :param column: Column to process.\n",
    "    :param cache_file: Path to the cache file.\n",
    "    :return: List of dense embeddings and corresponding texts.\n",
    "    \"\"\"\n",
    "    # Compute file hash\n",
    "    file_hash = compute_file_hash(input_csv)\n",
    "\n",
    "    # Load cached embeddings if available\n",
    "    embeddings, texts = load_cached_embeddings(cache_file, file_hash)\n",
    "    if embeddings is not None and texts is not None:\n",
    "        print(\"Loaded cached embeddings.\")\n",
    "        return embeddings, texts\n",
    "\n",
    "    # Generate embeddings\n",
    "    print(\"Generating dense embeddings for the database...\")\n",
    "    model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)  # Use FP16 for faster computation\n",
    "    df = pd.read_csv(input_csv)\n",
    "    texts = df[column].dropna().tolist()  # Drop NaN values and convert to list\n",
    "\n",
    "    embeddings = []\n",
    "    batch_size = 3  # Adjust batch size based on available memory\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\", unit=\"batch\", total=total_batches):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        print(f\"\\nProcessing batch {i // batch_size + 1}/{total_batches}...\")  # Verbose output for each batch\n",
    "        with torch.no_grad():\n",
    "            # Generate dense embeddings\n",
    "            batch_embeddings = model.encode(batch_texts, batch_size=batch_size, max_length=512)['dense_vecs']\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        print(f\"Batch {i // batch_size + 1} completed. Processed {len(batch_texts)} texts.\")\n",
    "\n",
    "    # Save embeddings to cache\n",
    "    save_embeddings_to_cache(cache_file, file_hash, embeddings, texts)\n",
    "    print(\"Embeddings generation completed and saved to cache.\")\n",
    "\n",
    "    return embeddings, texts\n",
    "\n",
    "def search_database(query, embeddings, texts, top_k=5):\n",
    "    \"\"\"\n",
    "    Searches the database for the most similar entries to the query using dense embeddings.\n",
    "    :param query: User's query in natural language.\n",
    "    :param embeddings: Precomputed dense embeddings for the database.\n",
    "    :param texts: Corresponding texts for the embeddings.\n",
    "    :param top_k: Number of top results to return.\n",
    "    :return: List of top-k results with their similarity scores.\n",
    "    \"\"\"\n",
    "    print(\"Processing query...\")\n",
    "    model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)  # Use FP16 for faster computation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate dense embedding for the query\n",
    "        query_embedding = model.encode([query], max_length=512)['dense_vecs'][0]\n",
    "\n",
    "    # Convert embeddings to numpy.ndarray for better performance\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    # Convert embeddings and query_embedding to PyTorch tensors\n",
    "    embeddings_tensor = torch.tensor(embeddings)\n",
    "    query_embedding_tensor = torch.tensor(query_embedding).unsqueeze(0)  # Expand to 2-D\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = embeddings_tensor @ query_embedding_tensor.T\n",
    "    top_results = torch.topk(similarities.squeeze(), k=top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx, score in zip(top_results.indices, top_results.values):\n",
    "        results.append((texts[idx], score.item(), idx.item()))  # Include the index for CSV export\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_html(query, results, output_file=\"output/search_results.html\", input_csv=None):\n",
    "    \"\"\"\n",
    "    Generates an HTML file to display search results in the 'output' folder.\n",
    "    \"\"\"\n",
    "    ensure_output_folder_exists()\n",
    "    print(\"Generating HTML file for search results...\")\n",
    "\n",
    "    # Load the original CSV to get additional information (e.g., condicion_localizacion)\n",
    "    df = pd.read_csv(input_csv) if input_csv else None\n",
    "\n",
    "    # Define the template directory and load the template\n",
    "    env = Environment(loader=FileSystemLoader(\".\"))  # Current directory\n",
    "    template = env.from_string(\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>Search Results</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                padding: 0;\n",
    "                background-color: #f4f4f9;\n",
    "            }\n",
    "            h1 {\n",
    "                color: #333;\n",
    "            }\n",
    "            .result {\n",
    "                border: 1px solid #ddd;\n",
    "                border-radius: 5px;\n",
    "                padding: 15px;\n",
    "                margin-bottom: 10px;\n",
    "                background-color: #fff;\n",
    "            }\n",
    "            .result h2 {\n",
    "                margin: 0;\n",
    "                font-size: 18px;\n",
    "                color: #007BFF;\n",
    "            }\n",
    "            .result p {\n",
    "                margin: 5px 0;\n",
    "                font-size: 14px;\n",
    "                color: #555;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Search Results for: \"{{ query }}\"</h1>\n",
    "        <p>Showing {{ results|length }} results:</p>\n",
    "        {% for result in results %}\n",
    "        <div class=\"result\">\n",
    "            <h2>Result {{ result.rank }}</h2>\n",
    "            <p><strong>Similarity:</strong> {{ \"%.4f\"|format(result.score) }}</p>\n",
    "            <p><strong>Condition:</strong> {{ result.condicion }}</p>\n",
    "            <p>{{ result.text }}</p>\n",
    "        </div>\n",
    "        {% endfor %}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\")\n",
    "\n",
    "    # Prepare the results with additional information from the CSV\n",
    "    enriched_results = []\n",
    "    for rank, (text, score, idx) in enumerate(results, start=1):\n",
    "        condicion = df.iloc[idx]['condicion_localizacion'] if df is not None else \"N/A\"\n",
    "        enriched_results.append({\"rank\": rank, \"text\": text, \"score\": score, \"condicion\": condicion})\n",
    "\n",
    "    # Render the HTML content\n",
    "    html_content = template.render(query=query, results=enriched_results)\n",
    "\n",
    "    # Write the HTML content to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(f\"HTML file generated: {output_file}\")\n",
    "\n",
    "def export_to_csv(results, original_csv, output_csv=\"output/filtered_results.csv\"):\n",
    "    \"\"\"\n",
    "    Exports the search results to a new CSV file in the 'output' folder.\n",
    "    \"\"\"\n",
    "    ensure_output_folder_exists()\n",
    "    print(\"Exporting results to CSV...\")\n",
    "    df = pd.read_csv(original_csv)\n",
    "    indices = [idx for _, _, idx in results]  # Extract indices from results\n",
    "    filtered_df = df.iloc[indices]  # Filter rows by indices\n",
    "    filtered_df.to_csv(output_csv, index=False)  # Save to a new CSV file\n",
    "    print(f\"Results exported to {output_csv}\")\n",
    "\n",
    "def save_with_timestamp(file_path):\n",
    "    \"\"\"\n",
    "    Saves a file with a timestamp in its filename inside the 'output' folder.\n",
    "    :param file_path: Path to the file to save.\n",
    "    \"\"\"\n",
    "    output_folder = ensure_output_folder_exists()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base, ext = os.path.splitext(file_path)\n",
    "    timestamped_file = os.path.join(output_folder, f\"{os.path.basename(base)}_{timestamp}{ext}\")\n",
    "    os.rename(file_path, timestamped_file)\n",
    "    print(f\"File saved with timestamp: {timestamped_file}\")\n",
    "    return timestamped_file\n",
    "\n",
    "def main():\n",
    "    # Set GPU memory limit\n",
    "    set_memory_limit(memory_fraction=1.0)\n",
    "\n",
    "    input_csv = \"data/repd_vp_cedulas_principal.csv\"\n",
    "    column = \"descripcion_desaparicion\"\n",
    "    cache_file = \"data/database_embeddings.pkl\"\n",
    "\n",
    "    # Preguntar al usuario si desea aplicar un filtro\n",
    "    apply_filter_option = input(\"Do you want to apply a filter to the dataset? (yes/no): \").strip().lower()\n",
    "    if apply_filter_option == \"yes\":\n",
    "        filtered_csv = \"data/filtered_dataset.csv\"\n",
    "        input_csv = apply_filter(input_csv, filtered_csv)\n",
    "        cache_file = \"data/filtered_embeddings.pkl\"  # Cambiar al archivo de caché para la base de datos filtrada\n",
    "\n",
    "    # Generar o cargar embeddings\n",
    "    embeddings, texts = generate_embeddings_for_database(input_csv, column, cache_file)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Ask the user for a query\n",
    "            query = input(\"\\nEnter your query (or type 'exit' to quit): \")\n",
    "            if query.lower() == \"exit\":\n",
    "                print(\"\\nExiting... Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # Ask the user how many results to display\n",
    "            try:\n",
    "                top_k = int(input(\"How many results do you want to display? \"))\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Showing 5 results by default.\")\n",
    "                top_k = 5\n",
    "\n",
    "            # Search the database\n",
    "            results = search_database(query, embeddings, texts, top_k=top_k)\n",
    "\n",
    "            # Generate an HTML file with the results\n",
    "            output_html = \"output/search_results.html\"\n",
    "            generate_html(query, results, output_html, input_csv=input_csv)\n",
    "            save_with_timestamp(output_html)\n",
    "\n",
    "            # Export results to a new CSV\n",
    "            output_csv = \"output/filtered_results.csv\"\n",
    "            export_to_csv(results, input_csv, output_csv)\n",
    "            save_with_timestamp(output_csv)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nKeyboard interrupt detected. Returning to options...\\n\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
